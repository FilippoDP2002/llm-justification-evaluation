# Project: Qualitative Analysis and Comparative Evaluation Methods for LLM-Generated Texts

**Course:** Natural Language Processing - [Bocconi University / Data Science]
**Author(s):** Filippo Dario Paolucci, Luca D'ambrosio, Edoardo Picazio
**Date:** April 2025

---

## 1. Introduction and Motivation

Language Models (LMs) can generate complex text, including explanations for their answers. However, assessing the intrinsic quality of these generated texts—their logical coherence, reasoning depth, relevance, and factual accuracy—is challenging and goes beyond simple task-specific accuracy metrics. This project focuses on the qualitative analysis of justifications produced by different LLMs and a comparative evaluation of methods used to assess these texts. Understanding how to effectively analyze these outputs and select appropriate evaluation methodologies is crucial for the reliable use of LLMs.

## 2. Project Goals

The main objectives of this project are:

* To perform a qualitative analysis of justifications generated by selected LLMs (e.g.,Qwen2.5:1.5B, Qwen2.5:14B, Deepseek-r1:1.5B, Deepseek-r1:14B) for specific tasks (e.g., GMAT-style verbal reasoning), focusing on argumentative structure, coherence, and relevance.
* To compare the effectiveness of different automated evaluation approaches for justification quality:
    * Methods based on traditional NLP metrics (e.g., BLEU, TF-IDF cosine similarity).
    * Methods based on  semantic and logical evaluation (e.g. SBERT).
    * Methods based on semantic and logical evaluation using a more powerful LLM (e.g., Deepseek-r1 671B) as an "expert judge".
* To contextualize the qualitative analysis by evaluating the base task accuracy of the models.
* To derive insights into the nature of texts generated by different LLMs and the reliability of various automated text evaluation methods.

## 3. Methodology Overview

The project follows these main phases:

1.  **Model and Data Selection:** Select representative LLMs (Gemma 1B, Llama 8B, Phi-4 14.5B) and a suitable evaluation dataset (e.g., GMAT verbal reasoning).
2.  **Data Generation:** Query the selected LLMs to collect their responses and, more importantly, their detailed justifications for the task.
3.  **Justification Analysis and Evaluation:** Analyze the collected justifications using parallel approaches:
    * **Traditional Syntactic Metrics:** Apply metrics like BLEU and TF-IDF cosine similarity to quantify aspects like lexical overlap and surface semantic similarity against reference justifications (if available). Analyze the limitations of these metrics.
    *  **Traditional Semantic Mtetrics:** Apply sentence embeddings like SBERT and MathBERT to quantify semantic similarity against reference justifications, measured through cosine similarity.
    * **LLM Judge Evaluation:** Employ a larger LLM (Deepseek-r1 671B) with engineered prompts to assess logical correctness, coherence, completeness, and persuasiveness. Explore various prompting strategies.
4.  **Comparative Analysis:** Compare the results from the traditional vs. LLM-judge evaluation methods. Analyze correlations and how model accuracy relates to justification quality according to different metrics.

## 4. Models & Data

* **Models Used:** Qwen2.5:1.5B, Qwen2.5:14B, Deepseek-r1:1.5B, Deepseek-r1:14B
* **Task Dataset:** **WIP**

### Data Handling

This repository uses **Git Large File Storage (LFS)** to handle large data files, such as the raw dataset and the LLM-generated outputs.

* **Requirement:** Please ensure you have Git LFS installed. You can download and install it from [https://git-lfs.com/](https://git-lfs.com/).
* **Cloning:** Clone the repository as usual. Git LFS will automatically download the large files tracked during the checkout process.
    ```bash
    git clone git@github.com:edopica/llm-justification-evaluation.git
    ```
* **File Locations:**
    * Raw input data should be placed in `data/raw/` [Add specific instructions here if the raw dataset needs manual placement even with LFS, e.g., if it's downloaded separately first].
    * Generated data (outputs from scripts like `01_generate_data.py`) will be saved in `data/generated/` and should be tracked by Git LFS. Make sure to configure LFS to track the relevant file types (e.g., `git lfs track "data/generated/*.csv"`).

## 5. Project Structure
**WIP**

## 6. Setup and Usage

1.  **Install Git LFS:** Ensure Git LFS is installed globally (see Data Handling section).

2.  **Clone the repository:**
    ```bash
    git clone git@github.com:FilippoDP2002/llm-justification-evaluation.git
    cd llm-justification-evaluation
    ```

3.  **Create a virtual environment (recommended):**
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows use `venv\Scripts\activate`
    ```

4.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

5.  **Verify Data:** After cloning, check that the large files in `data/raw/` and `data/generated/` (if any already exist in the repo) have been downloaded correctly by Git LFS. If you need to place the raw dataset manually, do so now according to the instructions in the Data Handling section.

6.  **Run Scripts:** Execute the scripts located in the `scripts/` directory as needed. Remember to add and commit any large generated output files using Git LFS.

## 7. Results

**[WIP]**

## 8. License

This project is licensed under the **GNU General Public License v3.0**. See the [LICENSE](LICENSE) file for details.

---
