{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0650416",
   "metadata": {},
   "source": [
    "## Description\n",
    "This notebook cleanes the dataset of models' responses to critical reasoning question pool:\n",
    "- Merges datasets responses generated from different runs of the script\n",
    "- Pivots horizontally the dataset\n",
    "- Divides the <think> part from the actual response in deepseek-r1's answers\n",
    "- Divides the different sections of the response in ##Reasoning and ##Solution, extracting them with tailored regexes\n",
    "- Assigns a score 0-1 based on whether the structure response instructions were followed or not\n",
    "- Adds colums from original datasets for additional information, for proper evaluation\n",
    "- Assign a score 0-1 based on whether the models aswered correctly or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d23f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "desktop_path = os.path.join(os.path.expanduser(\"~\"), \"llm-justification-evaluation\", \"Data_cleaning_cosine_calculation_semantic_and_analysis\")\n",
    "os.chdir(desktop_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bad65d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_luca=pd.read_csv('Models_answers/math_answers_luca.csv')\n",
    "df_final_to_merge=pd.read_csv('Models_answers/math_answers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c0404e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_combined = pd.concat([df_luca, df_final_to_merge]).reset_index(drop=True)\n",
    "df_final_combined = df_final_combined[df_final_combined['response'] != 'Error: Failed to connect to Ollama. Please check that Ollama is downloaded, running and accessible. https://ollama.com/download']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24f3bff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_merged_1= df_final_combined[df_final_combined['model'] == 'deepseek-r1:14b']\n",
    "df_final_merged_1 = df_final_merged_1.drop_duplicates(subset=['uuid'], keep='first')\n",
    "df_final_merged_2= df_final_combined[df_final_combined['model'] == 'deepseek-r1:1.5b']\n",
    "df_final_merged_2 = df_final_merged_2.drop_duplicates(subset=['uuid'], keep='first')\n",
    "df_final_combined = df_final_combined[~df_final_combined['model'].isin(['deepseek-r1:14b', 'deepseek-r1:1.5b'])]\n",
    "df_final_combined = pd.concat([df_final_combined, df_final_merged_2, df_final_merged_1]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7291a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_math_analysis=df_final_combined.copy()\n",
    "valid_uuids = df_math_analysis['uuid'].value_counts()\n",
    "valid_uuids = valid_uuids[valid_uuids == 4].index\n",
    "df_math_analysis = df_math_analysis[df_math_analysis['uuid'].isin(valid_uuids)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1de99908",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_df = df_math_analysis.pivot(index='uuid', columns='model', values='response')\n",
    "\n",
    "def split_think(text):\n",
    "    if isinstance(text, str):\n",
    "        match = re.search(r\"<think>(.*?)</think>\", text, re.DOTALL)\n",
    "        if match:\n",
    "            think_part = match.group(1).strip()\n",
    "            response_part = text.replace(match.group(0), \"\").strip()\n",
    "            return pd.Series([think_part, response_part])\n",
    "    return pd.Series([\"\", text])\n",
    "\n",
    "response_df[['deepseek-r1:1.5b_think', 'deepseek-r1:1.5b']] = response_df['deepseek-r1:1.5b'].apply(split_think)\n",
    "response_df[['deepseek-r1:14b_think', 'deepseek-r1:14b']] = response_df['deepseek-r1:14b'].apply(split_think)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4523bd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df = df_math_analysis.pivot(index='uuid', columns='model', values='time_taken_seconds')\n",
    "time_df.columns = [f\"{col}_time\" for col in time_df.columns]\n",
    "df_math_analysis = pd.concat([response_df, time_df], axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25d9e1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions = pd.read_csv('Additional_information_datasets/math_questions_pool.csv')\n",
    "df_questions = df_questions[['uuid', 'problem', 'problem_type', 'source']]\n",
    "df_math_analysis = df_math_analysis.merge(df_questions, on='uuid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61565512",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv('Additional_information_datasets/OpenR1-Math-220k_for_answers.csv')\n",
    "df_solutions = df1[['uuid', 'solution', 'answer']]\n",
    "df_math_analysistrial = df_math_analysis.merge(df_solutions, on='uuid', how='left')\n",
    "df_math_analysistrial\n",
    "df_math_analysistrial['solution'].value_counts()\n",
    "df_math_analysistrial = df_math_analysistrial[df_math_analysistrial['solution'].map(df_math_analysistrial['solution'].value_counts()) == 1]\n",
    "df_math_analysistrial['solution'].value_counts()\n",
    "\n",
    "df_math_analysistrial_sample = df_math_analysistrial.sample(n=500, random_state=40)\n",
    "df_math_analysistrial_sample.reset_index(drop=True, inplace=True)\n",
    "df_math_analysistrial_sample['answer'] = df_math_analysistrial_sample['answer'].str.replace(r'\\\\text\\s*{\\s*([A-D])\\s*}', r'\\1', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f153b227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_and_score_model_column(df, model_name):\n",
    "    reasoning_col = f\"{model_name}_reasoning\"\n",
    "    solution_col = f\"{model_name}_solution\"\n",
    "    score_col = f\"{model_name}_structure_score\"\n",
    "\n",
    "    reasoning_list = []\n",
    "    solution_list = []\n",
    "    structure_score_list = []\n",
    "\n",
    "    for text in df[model_name]:\n",
    "        text = str(text)\n",
    "        reasoning = \"\"\n",
    "        solution = \"\"\n",
    "        score = 1 \n",
    "\n",
    "        has_reasoning_heading = \"## Reasoning\" in text\n",
    "        has_solution_heading = \"## Solution\" in text\n",
    "        has_answer_heading = \"**Answer:**\" in text\n",
    "        boxed_answer_match = re.search(r\"\\\\boxed\\{([A-D])\\}\", text)\n",
    "\n",
    "        if has_reasoning_heading:\n",
    "            reasoning_match = re.search(\n",
    "                r\"##\\s*Reasoning\\s*(.*?)(?=##\\s*Solution|\\*\\*Answer:|\\\\boxed\\{[A-D]\\}|$)\",\n",
    "                text,\n",
    "                re.DOTALL | re.IGNORECASE\n",
    "            )\n",
    "            if reasoning_match:\n",
    "                reasoning = reasoning_match.group(1).strip()\n",
    "        else:\n",
    "            fallback_reasoning_match = re.search(\n",
    "                r\"^(.*?)(?=##\\s*Solution|\\*\\*Answer:|\\\\boxed\\{[A-D]\\})\",\n",
    "                text,\n",
    "                re.DOTALL | re.IGNORECASE\n",
    "            )\n",
    "            if fallback_reasoning_match:\n",
    "                reasoning = fallback_reasoning_match.group(1).strip()\n",
    "\n",
    "        solution_match = re.search(r\"##\\s*Solution\\s*([A-D])\\s*$\", text, re.MULTILINE)\n",
    "        if solution_match:\n",
    "            solution = solution_match.group(1)\n",
    "            if has_reasoning_heading:\n",
    "                score = 0\n",
    "        else:\n",
    "            answer_match = re.search(r\"\\*\\*Answer:\\*\\*\\s*([A-D])\", text, re.IGNORECASE)\n",
    "            if answer_match:\n",
    "                solution = answer_match.group(1)\n",
    "            elif boxed_answer_match:\n",
    "                solution = boxed_answer_match.group(1)\n",
    "            else:\n",
    "                raw_match = re.search(r\"(##\\s*Solution|\\*\\*Answer:\\*\\*|\\[\\s*\\\\boxed\\{[A-D]\\}\\s*\\])\\s*(.*)\", text, re.DOTALL | re.IGNORECASE)\n",
    "                if raw_match:\n",
    "                    solution = raw_match.group(2).strip()\n",
    "\n",
    "        if reasoning.strip() == '' or solution.strip() == '':\n",
    "            reasoning = text\n",
    "            solution = text\n",
    "            score = 1 \n",
    "\n",
    "        reasoning_list.append(reasoning)\n",
    "        solution_list.append(solution)\n",
    "        structure_score_list.append(score)\n",
    "\n",
    "    df[reasoning_col] = reasoning_list\n",
    "    df[solution_col] = solution_list\n",
    "    df[score_col] = structure_score_list\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "models = [\n",
    "    \"deepseek-r1:1.5b\",\n",
    "    \"deepseek-r1:14b\",\n",
    "    \"qwen2.5:1.5b\",\n",
    "    \"qwen2.5:14b\"\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    df_math_analysistrial_sample = parse_and_score_model_column(df_math_analysistrial_sample, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c3e0731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_solution(text):\n",
    "    if not isinstance(text, str):\n",
    "        return float('nan')\n",
    "    \n",
    "    text = text.strip()\n",
    "    pattern = re.compile(r'(?:\\\\boxed\\{\\\\text\\{([A-Da-d])\\}\\}|\\\\text\\{([A-Da-d])\\}|^([A-Da-d]):|([A-Da-d]))')\n",
    "    \n",
    "    match = pattern.search(text)\n",
    "    if match:\n",
    "        letter = next(g for g in match.groups() if g is not None)\n",
    "        return letter.upper()\n",
    "    \n",
    "    return float('nan')\n",
    "\n",
    "for model_name in models:\n",
    "    col = f\"{model_name}_solution\"\n",
    "    df_math_analysistrial_sample[col] = df_math_analysistrial_sample[col].apply(clean_solution)\n",
    "df_math_analysistrial_sample['answer'] = df_math_analysistrial_sample['answer'].apply(clean_solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c7c1ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_math_analysistrial_sample = df_math_analysistrial_sample.drop(columns=['deepseek-r1:1.5b_think', 'deepseek-r1:14b_think'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e047cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"deepseek-r1:1.5b\", \"deepseek-r1:14b\", \"qwen2.5:1.5b\", \"qwen2.5:14b\"]\n",
    "for model_name in model_names:\n",
    "    sol_col = f\"{model_name}_solution\"\n",
    "    ans_col = 'answer'\n",
    "    correct_col = f\"{model_name}_correct\"\n",
    "    \n",
    "    df_math_analysistrial_sample[correct_col] = np.where(df_math_analysistrial_sample[sol_col] == df_math_analysistrial_sample[ans_col], 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59ec1bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_math_analysistrial_sample.to_csv('NLP_analysis/math_analysis.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
