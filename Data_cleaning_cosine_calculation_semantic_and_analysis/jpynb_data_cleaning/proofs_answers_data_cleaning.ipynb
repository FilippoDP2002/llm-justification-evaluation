{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b44f1d3e",
   "metadata": {},
   "source": [
    "## Description\n",
    "This notebook cleanes the dataset of models' responses to the proofs question pool:\n",
    "- Merges datasets responses generated from different runs of the script\n",
    "- Pivots horizontally the dataset\n",
    "- Divides the <think> part from the actual response in deepseek-r1's answers\n",
    "- Divides the different sections of the response in ##Reasoning and ##Solution, extracting them with tailored regexes\n",
    "- Assigns a score 0-1 based on whether the structure response instructions were followed or not\n",
    "- Adds colums from original datasets for additional information, for proper evaluation\n",
    "- Assign a score 0-1 based on whether the models aswered correctly or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1ea313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "desktop_path = os.path.join(os.path.expanduser(\"~\"), \"llm-justification-evaluation\", \"Data_cleaning_cosine_calculation_semantic_and_analysis\")\n",
    "os.chdir(desktop_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96fa001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proofs= pd.read_csv('Models_answers/proofs_answers.csv')\n",
    "df_proofs = df_proofs.drop_duplicates(subset=['uuid', 'model'], keep='last')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e297c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proofs_response = df_proofs.pivot(index='uuid', columns='model', values='response').reset_index()\n",
    "\n",
    "def split_think(text):\n",
    "    if isinstance(text, str):\n",
    "        match = re.search(r\"<think>(.*?)</think>\", text, re.DOTALL)\n",
    "        if match:\n",
    "            think_part = match.group(1).strip()\n",
    "            response_part = text.replace(match.group(0), \"\").strip()\n",
    "            return pd.Series([think_part, response_part])\n",
    "    return pd.Series([\"\", text])\n",
    "\n",
    "df_proofs_response[['deepseek-r1:1.5b_think', 'deepseek-r1:1.5b']] = df_proofs_response['deepseek-r1:1.5b'].apply(split_think)\n",
    "df_proofs_response[['deepseek-r1:14b_think', 'deepseek-r1:14b']] = df_proofs_response['deepseek-r1:14b'].apply(split_think)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e2de304",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proofs_time= df_proofs.pivot(index='uuid', columns='model', values='time_taken_seconds').reset_index()\n",
    "df_proofs_time.columns = [f\"{col}_time\" for col in df_proofs_time.columns]\n",
    "df_proofs_analysis = pd.concat([df_proofs_response, df_proofs_time], axis=1).reset_index()\n",
    "df_proofs_analysis = df_proofs_analysis.drop(columns=['uuid_time', 'index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c469bb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('Additional_information_datasets/OpenR1-Math-220k_for_answers.csv')\n",
    "data_to_merge=data[['uuid','answer', 'solution', 'problem_type', 'problem']]\n",
    "df_proofs_analysis = df_proofs_analysis.merge(data_to_merge, on='uuid', how='left')\n",
    "df_proofs_analysis = df_proofs_analysis.drop(columns=[col for col in df_proofs_analysis.columns if '_think' in col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ca6dad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_and_score_model_column(df, model_name):\n",
    "    reasoning_col = f\"{model_name}_reasoning\"\n",
    "    solution_col = f\"{model_name}_solution\"\n",
    "    score_col = f\"{model_name}_structure_score\"\n",
    "\n",
    "    reasoning_list = []\n",
    "    solution_list = []\n",
    "    structure_score_list = []\n",
    "\n",
    "    for text in df_proofs_analysis[model_name]:\n",
    "        text = str(text)\n",
    "        reasoning = \"\"\n",
    "        solution = \"\"\n",
    "        score = 1 \n",
    "\n",
    "        has_reasoning_heading = \"## Reasoning\" in text\n",
    "        has_solution_heading = \"## Solution\" in text\n",
    "        has_answer_heading = \"**Answer:**\" in text\n",
    "        boxed_answer_match = re.search(r\"\\\\boxed\\{([A-D])\\}\", text)\n",
    "\n",
    "        if has_reasoning_heading:\n",
    "            reasoning_match = re.search(\n",
    "                r\"##\\s*Reasoning\\s*(.*?)(?=##\\s*Solution|\\*\\*Answer:|\\\\boxed\\{[A-D]\\}|$)\",\n",
    "                text,\n",
    "                re.DOTALL | re.IGNORECASE\n",
    "            )\n",
    "            if reasoning_match:\n",
    "                reasoning = reasoning_match.group(1).strip()\n",
    "        else:\n",
    "            fallback_reasoning_match = re.search(\n",
    "                r\"^(.*?)(?=##\\s*Solution|\\*\\*Answer:|\\\\boxed\\{[A-D]\\})\",\n",
    "                text,\n",
    "                re.DOTALL | re.IGNORECASE\n",
    "            )\n",
    "            if fallback_reasoning_match:\n",
    "                reasoning = fallback_reasoning_match.group(1).strip()\n",
    "\n",
    "        solution_match = re.search(r\"##\\s*Solution\\s*([A-D])\\s*$\", text, re.MULTILINE)\n",
    "        if solution_match:\n",
    "            solution = solution_match.group(1)\n",
    "            if has_reasoning_heading:\n",
    "                score = 0\n",
    "        else:\n",
    "            answer_match = re.search(r\"\\*\\*Answer:\\*\\*\\s*([A-D])\", text, re.IGNORECASE)\n",
    "            if answer_match:\n",
    "                solution = answer_match.group(1)\n",
    "            elif boxed_answer_match:\n",
    "                solution = boxed_answer_match.group(1)\n",
    "            else:\n",
    "                raw_match = re.search(r\"(##\\s*Solution|\\*\\*Answer:\\*\\*|\\[\\s*\\\\boxed\\{[A-D]\\}\\s*\\])\\s*(.*)\", text, re.DOTALL | re.IGNORECASE)\n",
    "                if raw_match:\n",
    "                    solution = raw_match.group(2).strip()\n",
    "\n",
    "        if reasoning.strip() == '' or solution.strip() == '':\n",
    "            reasoning = text\n",
    "            solution = text\n",
    "            score = 1 \n",
    "\n",
    "        reasoning_list.append(reasoning)\n",
    "        solution_list.append(solution)\n",
    "        structure_score_list.append(score)\n",
    "\n",
    "    df[reasoning_col] = reasoning_list\n",
    "    df[solution_col] = solution_list\n",
    "    df[score_col] = structure_score_list\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "models = [\n",
    "    \"deepseek-r1:1.5b\",\n",
    "    \"deepseek-r1:14b\",\n",
    "    \"qwen2.5:1.5b\",\n",
    "    \"qwen2.5:14b\"\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    df_proofs_analysis = parse_and_score_model_column(df_proofs_analysis, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "088b35c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proofs_analysis.to_csv('NLP_analysis/proofs_analysis.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
