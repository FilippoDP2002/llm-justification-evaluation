{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "307e1555",
   "metadata": {},
   "source": [
    "## Description\n",
    "This notebook cleanes the dataset of models' responses to the critical reasoning question pool:\n",
    "- Merges datasets responses generated from different runs of the script\n",
    "- Pivots horizontally the dataset\n",
    "- Divides the <think> part from the actual response in deepseek-r1's answers\n",
    "- Divides the different sections of the response in ##Reasoning and ##Solution, extracting them with tailored regexes\n",
    "- Assigns a score 0-1 based on whether the structure response instructions were followed or not\n",
    "- Adds colums from original datasets for additional information, for proper evaluation\n",
    "- Assign a score 0-1 based on whether the models aswered correctly or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94a5c1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "desktop_path = os.path.join(os.path.expanduser(\"~\"), \"Desktop\", \"Data_cleaning_cosine_calculation_semantic_and_analysis\")\n",
    "os.chdir(desktop_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77cff188",
   "metadata": {},
   "outputs": [],
   "source": [
    "crit_reas_pippo= pd.read_csv('Models_answers/critical_reasoning_answers.csv')\n",
    "crit_reas_luca= pd.read_csv('Models_answers/critical_reasoning_answers_luca.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cdb370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "crit_reas_pippo = crit_reas_pippo[~crit_reas_pippo['model'].isin(['deepseek-r1:1.5b', 'qwen2.5:1.5b'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73dff246",
   "metadata": {},
   "outputs": [],
   "source": [
    "crit_reas_pippo = crit_reas_pippo.drop_duplicates(subset=['QuestionID', 'model'], keep='last')\n",
    "crit_reas=pd.concat([crit_reas_pippo, crit_reas_luca], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b4ae443",
   "metadata": {},
   "outputs": [],
   "source": [
    "crit_reas_response = crit_reas.pivot(index='QuestionID', columns='model', values='response').reset_index()\n",
    "\n",
    "import re\n",
    "def split_think(text):\n",
    "    if isinstance(text, str):\n",
    "        match = re.search(r\"<think>(.*?)</think>\", text, re.DOTALL)\n",
    "        if match:\n",
    "            think_part = match.group(1).strip()\n",
    "            response_part = text.replace(match.group(0), \"\").strip()\n",
    "            return pd.Series([think_part, response_part])\n",
    "    return pd.Series([\"\", text])\n",
    "\n",
    "crit_reas_response[['deepseek-r1:1.5b_think', 'deepseek-r1:1.5b']] = crit_reas_response['deepseek-r1:1.5b'].apply(split_think)\n",
    "crit_reas_response[['deepseek-r1:14b_think', 'deepseek-r1:14b']] = crit_reas_response['deepseek-r1:14b'].apply(split_think)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51dd0966",
   "metadata": {},
   "outputs": [],
   "source": [
    "crit_reas_time= crit_reas.pivot(index='QuestionID', columns='model', values='time_taken_seconds').reset_index()\n",
    "crit_reas_time.columns = [f\"{col}_time\" for col in crit_reas_time.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e00e7322",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crit_analysis = pd.concat([crit_reas_response, crit_reas_time], axis=1).reset_index()\n",
    "df_crit_analysis = df_crit_analysis.drop(columns=['QuestionID_time', 'index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "266967d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('Additional_information_datasets/critical_reasoning_questions.csv')\n",
    "data_to_merge=data[['QuestionID','Answer', 'Solution', 'QuestionText']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63c443f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crit_analysis = df_crit_analysis.merge(data_to_merge, on='QuestionID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51482eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_argument_answer(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\", \"\", 1\n",
    "\n",
    "    text = text.strip()\n",
    "    arg_marker = \"# Argument Construction\"\n",
    "    ans_marker = \"# Answer\"\n",
    "\n",
    "    arg_index = text.find(arg_marker)\n",
    "    ans_index = text.find(ans_marker)\n",
    "\n",
    "    if arg_index != -1 and ans_index != -1 and arg_index < ans_index:\n",
    "        evaluation = text[arg_index + len(arg_marker):ans_index].strip()\n",
    "        solution = text[ans_index + len(ans_marker):].strip()\n",
    "        return evaluation, solution, 0 \n",
    "\n",
    "    return text.strip(), text.strip(), 1\n",
    "\n",
    "\n",
    "model_names = [\"deepseek-r1:1.5b\", \"deepseek-r1:14b\", \"qwen2.5:1.5b\", \"qwen2.5:14b\"]\n",
    "\n",
    "for model_name in model_names:\n",
    "    source_col = model_name\n",
    "    eval_col = f\"{model_name}_evaluation\"\n",
    "    sol_col = f\"{model_name}_solution\"\n",
    "    score_col = f\"{model_name}_structure_score\"\n",
    "\n",
    "    results = df_crit_analysis[source_col].apply(split_argument_answer)\n",
    "    df_crit_analysis[[eval_col, sol_col, score_col]] = pd.DataFrame(results.tolist(), index=df_crit_analysis.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eab7afbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_crit_analysis = df_crit_analysis.drop(columns=[col for col in df_crit_analysis.columns if '_think' in col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80c87f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crit_analysis = df_crit_analysis[df_crit_analysis['Solution'] != 'Solution not found.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37dadec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_solution(text):\n",
    "    if not isinstance(text, str):\n",
    "        return np.nan\n",
    "\n",
    "    text = text.strip()\n",
    "    pattern = re.compile(\n",
    "        r'(?i)(?:\\\\boxed\\{\\\\text\\{([A-E])\\}\\}'      \n",
    "        r'|\\\\text\\{([A-E])\\}'                       \n",
    "        r'|\\*\\*([A-E])\\*\\*'                         \n",
    "        r'|\\(([A-E])\\)'                             \n",
    "        r'|#\\s*([A-E])'                             \n",
    "        r'|answer\\s*[:\\s]*([A-E])'                  \n",
    "        r'|final\\s+answer\\s*[:\\s]*([A-E])'          \n",
    "        r'|correct\\s+answer\\s+is\\s*[:\\s]*([A-E])'     \n",
    "        r'|^([A-E])$'                                 \n",
    "        r')'\n",
    "    )\n",
    "\n",
    "    match = pattern.search(text)\n",
    "    if match:\n",
    "        for group in match.groups():\n",
    "            if group:\n",
    "                return group.upper()\n",
    "\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "model_names = [\"deepseek-r1:1.5b\", \"deepseek-r1:14b\", \"qwen2.5:1.5b\", \"qwen2.5:14b\"]\n",
    "\n",
    "for model_name in model_names:\n",
    "    sol_col = f\"{model_name}_solution\"\n",
    "    df_crit_analysis[sol_col] = df_crit_analysis[sol_col].apply(clean_solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87d3efbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_solution_sums = {}\n",
    "for model_name in model_names:\n",
    "    sol_col = f\"{model_name}_solution\"\n",
    "    model_solution_sums[model_name] = df_crit_analysis[sol_col].value_counts().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95be1851",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crit_analysis = df_crit_analysis.rename(columns={f\"{model_name}_evaluation\": f\"{model_name}_reasoning\" for model_name in model_names})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6728fb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in model_names:\n",
    "    sol_col = f\"{model_name}_solution\"\n",
    "    ans_col = 'Answer'\n",
    "    correct_col = f\"{model_name}_correct\"\n",
    "    \n",
    "    df_crit_analysis[correct_col] = np.where(df_crit_analysis[sol_col] == df_crit_analysis[ans_col], 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e04680d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crit_analysis.to_csv('NLP_analysis/critical_reasoning_analysis.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
