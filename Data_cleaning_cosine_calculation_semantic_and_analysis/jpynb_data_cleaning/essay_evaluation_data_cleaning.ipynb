{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d901a99e",
   "metadata": {},
   "source": [
    "## Description\n",
    "This notebook cleanes the dataset of models' responses to the math question pool:\n",
    "- Merges datasets responses generated from different runs of the script\n",
    "- Pivots horizontally the dataset\n",
    "- Divides the <think> part from the actual response in deepseek-r1's answers\n",
    "- Divides the different sections of the response in ##Reasoning and ##Solution, extracting them with tailored regexes\n",
    "- Assigns a score 0-1 based on whether the structure response instructions were followed or not\n",
    "- Adds colums from original datasets for additional information, for proper evaluation\n",
    "- Assign a score 0-1 based on whether the models aswered correctly or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b85152c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "desktop_path = os.path.join(os.path.expanduser(\"~\"), \"llm-justification-evaluation\", \"Data_cleaning_cosine_calculation_semantic_and_analysis\")\n",
    "os.chdir(desktop_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92e2cc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_answers = pd.read_csv('Models_answers/essay_evaluation_answers.csv')\n",
    "essay_luca = pd.read_csv('Models_answers/essay_evaluation_answers_luca.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da98280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "essay_luca = essay_luca[essay_luca['response'] != \"Error: 1 validation error for GenerateRequest\\nmodel\\n  String should have at least 1 character [type=string_too_short, input_value='', input_type=str]\\n    For further information visit https://errors.pydantic.dev/2.11/v/string_too_short\"]\n",
    "essay_luca['response'].value_counts()\n",
    "essay_luca['response'].str.contains('Error:').sum()\n",
    "error_ids = essay_luca[essay_luca['response'].str.contains('Error:')]['QuestionID'].unique()\n",
    "error_ids\n",
    "essay_luca = essay_luca[~essay_luca['QuestionID'].isin(error_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17753fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_answers = eval_answers[eval_answers['model'] != 'deekseek-r1:1.5b']\n",
    "eval_answers = eval_answers.drop_duplicates(subset=['QuestionID', 'model'], keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd5e177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_answ_temp = eval_answers[~eval_answers['model'].isin(['qwen2.5:14b', 'deepseek-r1:14b'])].copy()\n",
    "eval_answ_temp = eval_answ_temp[~eval_answ_temp['QuestionID'].isin([1, 2, 3, 4, 5])]\n",
    "\n",
    "eval_answers = eval_answers[\n",
    "    (eval_answers['model'] == 'qwen2.5:14b') | \n",
    "    (eval_answers['model'] == 'deepseek-r1:14b')]\n",
    "eval_answers = pd.concat([eval_answ_temp, eval_answers], ignore_index=True)\n",
    "eval_answers = pd.concat([eval_answers, essay_luca], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c4b0418",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_answers_x = eval_answers.pivot(index='QuestionID', columns='model', values='response').reset_index()\n",
    "eval_answers_x.columns.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6b8dc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_answers_time = eval_answers.pivot(index='QuestionID', columns='model', values='time_taken_seconds').reset_index()\n",
    "eval_answers_time.columns = [f\"{col}_time\" for col in eval_answers_time.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c79504f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_answers = pd.concat([eval_answers_x, eval_answers_time], axis=1).reset_index()\n",
    "eval_answers = eval_answers.drop(columns=['QuestionID_time', 'index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37d090b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_think(text):\n",
    "    if isinstance(text, str):\n",
    "        match = re.search(r\"<think>(.*?)</think>\", text, re.DOTALL)\n",
    "        if match:\n",
    "            think_part = match.group(1).strip()\n",
    "            response_part = text.replace(match.group(0), \"\").strip()\n",
    "            return pd.Series([think_part, response_part])\n",
    "    return pd.Series([\"\", text])\n",
    "\n",
    "eval_answers[['deepseek-r1:1.5b_think', 'deepseek-r1:1.5b']] = eval_answers['deepseek-r1:1.5b'].apply(split_think)\n",
    "eval_answers[['deepseek-r1:14b_think', 'deepseek-r1:14b']] = eval_answers['deepseek-r1:14b'].apply(split_think)\n",
    "\n",
    "eval_answers = eval_answers.drop(columns=['deepseek-r1:1.5b_think', 'deepseek-r1:14b_think'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82c785fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_eval=pd.read_csv('Additional_information_datasets/ielts_essays_questions.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e52afb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_answers = eval_answers.merge(get_eval[['QuestionID', 'evaluation','prompt', 'essay']], on='QuestionID', how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64320e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_answers.columns = [col + '_reasoning' if col not in ['QuestionID', 'evaluation', 'prompt', 'essay'] and not col.endswith('_time') and not col.endswith('_score') else col for col in eval_answers.columns]\n",
    "eval_answers = eval_answers.rename(columns={'evaluation': 'Solution'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1baf8d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_band_score_structure(text):\n",
    "    if not isinstance(text, str):\n",
    "        return 1, float('nan')\n",
    "\n",
    "    required_sections = [\n",
    "        \"## Task Achievement\",\n",
    "        \"## Coherence and Cohesion\",\n",
    "        \"## Lexical Resource\",\n",
    "        \"## Grammatical Range and Accuracy\",\n",
    "        \"## Overall Band Score\",\n",
    "        \"## Feedback and Additional Comments\"\n",
    "    ]\n",
    "\n",
    "    structure_ok = all(section in text for section in required_sections)\n",
    "    score = 0 if structure_ok else 1\n",
    "\n",
    "    patterns = [\n",
    "        r\"Suggested\\s+(?:Overall\\s+)?Band\\s+Score\\s*:\\s*(?!\\s*\\()\"\n",
    "        r\"\\**\\s*([0-9](?:\\.\\d)?)\\s*\\**\",\n",
    "\n",
    "        r\"\\*\\*Overall\\s+Band\\s+Score:\\*\\*\\s*<\\s*(\\d+(?:\\.\\d+)?)\\s*>\",\n",
    "\n",
    "        r\"\\**\\s*Overall\\s+Band\\s+Score\\s*:\\s*<\\s*(\\d+(?:\\.\\d+)?)\\s*>\", \n",
    "\n",
    "        r\"\\boverall\\s+(?:band\\s+)?score\\s*(?:is|was|=|of)?\\s*[:\\-]?\\s*(\\d+(?:\\.\\d+)?)\",\n",
    "\n",
    "        r\"\\*\\*Overall\\s+Band\\s+Score\\*\\*:\\s*(?:.+?)\\((\\d+(?:\\.\\d+)?)\\)\",\n",
    "\n",
    "        r\"(?:has|have|was|were|is|are)?\\s*\"\n",
    "        r\"(?:awarded|received|achieved|got|obtained)?\\s*\"\n",
    "        r\"(?:an?\\s+)?([0-9](?:\\.\\d)?)\\s+band\\s+score\"\n",
    "    ]\n",
    "\n",
    "    band_score = float('nan')\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            try:\n",
    "                band_score = float(match.group(1))\n",
    "            except (ValueError, IndexError):\n",
    "                band_score = float('nan')\n",
    "            break\n",
    "\n",
    "    return score, band_score\n",
    "\n",
    "for model in eval_answers.columns:\n",
    "    if model.endswith('_reasoning'):\n",
    "        eval_answers[[f\"{model.replace('reasoning', '')}_structure_score\", f\"{model.replace('reasoning', '')}_band_score\"]] = eval_answers[model].apply(check_band_score_structure).apply(pd.Series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c3ab138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_band_score(text):\n",
    "    if not isinstance(text, str):\n",
    "        return float('nan')\n",
    "\n",
    "   \n",
    "    text = text.replace('\\xa0', ' ')          \n",
    "    text = re.sub(r'\\s+', ' ', text)          \n",
    "    text = text.strip()                       \n",
    "\n",
    "    pattern = r\"Suggested\\s*Overall\\s*Band\\s*Score\\s*:\\s*\\**\\s*(\\d+(?:\\.\\d+)?)\\s*\\**\"\n",
    "    match = re.search(pattern, text, re.IGNORECASE)\n",
    "\n",
    "    if match:\n",
    "        return float(match.group(1))\n",
    "    return float('nan')\n",
    "\n",
    "\n",
    "eval_answers['band_score_solution'] = eval_answers['Solution'].apply(extract_band_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79d879df",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_answers.to_csv('NLP_analysis/essay_evaluation_analysis.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
